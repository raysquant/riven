{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "def load_doc(path):\n",
    "    with open(path, 'r') as doc:\n",
    "        return doc.read()\n",
    "\n",
    "text = load_doc('data/zoom_424b4.htm.txt')\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def filter_spans(spans):\n",
    "    # Filter a sequence of spans so they don't contain overlaps\n",
    "    # For spaCy 2.1.4+: this function is available as spacy.util.filter_spans()\n",
    "    get_sort_key = lambda span: (span.end - span.start, -span.start)\n",
    "    sorted_spans = sorted(spans, key=get_sort_key, reverse=True)\n",
    "    result = []\n",
    "    seen_tokens = set()\n",
    "    for span in sorted_spans:\n",
    "        # Check for end - 1 here because boundaries are inclusive\n",
    "        if span.start not in seen_tokens and span.end - 1 not in seen_tokens:\n",
    "            result.append(span)\n",
    "        seen_tokens.update(range(span.start, span.end))\n",
    "    result = sorted(result, key=lambda span: span.start)\n",
    "    return result\n",
    "\n",
    "def extract_currency_relations(doc):\n",
    "    # Merge entities and noun chunks into one token\n",
    "    spans = list(doc.ents) + list(doc.noun_chunks)\n",
    "    spans = filter_spans(spans)\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for span in spans:\n",
    "            retokenizer.merge(span)\n",
    "\n",
    "    relations = []\n",
    "    for money in filter(lambda w: w.ent_type_ == \"MONEY\", doc):\n",
    "        if money.dep_ in (\"attr\", \"dobj\"):\n",
    "            subject = [w for w in money.head.lefts if w.dep_ == \"nsubj\"]\n",
    "            if subject:\n",
    "                subject = subject[0]\n",
    "                relations.append((subject, money))\n",
    "        elif money.dep_ == \"pobj\" and money.head.dep_ == \"prep\":\n",
    "            relations.append((money.head.head, money))\n",
    "    return relations\n",
    "\n",
    "def contains_info(sent):\n",
    "    if ('revenue') in sent.text.lower():\n",
    "        for ent in sent.ents:            \n",
    "            if ent.label_ in ('DATE', 'TIME', 'PERCENT', 'MONEY'):\n",
    "                return True\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.strip()\n",
    "    return re.sub(r\"(\\s+)\",r\" \",text)\n",
    "\n",
    "g = (clean_text(s.text) for s in doc.sents if contains_info(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    sent = nlp(next(g))\n",
    "    relations = extract_currency_relations(sent)\n",
    "    for r1, r2 in relations:\n",
    "        print(sent)\n",
    "        print(\"{:<10}\\t{}\\t{}\".format(r1.text, r2.ent_type_, r2.text))\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sentence:\", sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in sent.noun_chunks:\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Entities in the sentence:\")\n",
    "for ent in sent.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(sent, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in sent:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
